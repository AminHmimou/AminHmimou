# -*- coding: utf-8 -*-
"""Copie de Copie de Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VF71iPPJn7MqWSs6pqgbQY835Ub-s2i6
"""

#Installation de toutes les bibliothèques nécessaires
!pip -q install langchain
!pip -q install bitsandbytes accelerate xformers einops
!pip -q install datasets loralib sentencepiece
!pip -q install pypdf
!pip -q install sentence_transformers
!pip install chromadb
!pip install openai
!pip install tiktoken

#Importation des fonctions nécessaires
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from langchain.document_loaders import Docx2txtLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from huggingface_hub import notebook_login
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
from langchain import HuggingFacePipeline
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
import os
import sys

#Importation des documents
!mkdir docs

#Itération sur tous les documents importés en vérifiant les extentions afin de bien charger le contenu dans la liste "document"
document=[]
for file in os.listdir("docs"):
  if file.endswith(".pdf"):
    pdf_path="./docs/"+file
    loader=PyPDFLoader(pdf_path)
    document.extend(loader.load())
  elif file.endswith('.docx') or file.endswith('.doc'):
    doc_path="./docs/"+file
    loader=Docx2txtLoader(doc_path)
    document.extend(loader.load())
  elif file.endswith('.txt'):
    text_path="./docs/"+file
    loader=TextLoader(text_path)
    document.extend(loader.load())

document

len(document)

#Définir les paramètres pour diviser le contenu de la liste "document" en des morceaux
document_splitter=CharacterTextSplitter(separator='\n', chunk_size=500, chunk_overlap=100)

#Diviser les documents
document_chunks=document_splitter.split_documents(document)

len(document_chunks)

document_chunks[0]

document_chunks[1]

document_chunks[100]

#Embeddings pour représenter le contenu textuel
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

os.environ["OPENAI_API_KEY"]="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"

embeddings = OpenAIEmbeddings()

embeddings

#Construire une base de données à l'aide de chroma en se basant sur les embeddings
vectordb=Chroma.from_documents(document_chunks,embedding=embeddings, persist_directory='./data')

vectordb.persist()

#le modèle GPT_3.5 pour créer un objet ChatOpenIA
llm=ChatOpenAI(temperature=0.7, model_name='gpt-3.5-turbo')

llm

#Initialiser une mémoire de conversation pour stocker l'historique de converstaion
memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True)

#Création de le Chaine du Q/A
pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm,
                                             retriever=vectordb.as_retriever(search_kwargs={'k':6}),
                                             verbose=False, memory=memory)

result=pdf_qa({"question":"de quoi parle ce document"})

result['answer']

#initialisation du chatbot
import sys
import time
print('---------------------------------------------------------------------------------')
print('Welcome to Polychat. How can I help you ?')
print('---------------------------------------------------------------------------------')

while True:
    query = input(f"Question:")

    if query.lower() in ["exit", "quit", "q", "f"]:
        print('Exiting')
        sys.exit()

    if query == '':
        continue

    start_time = time.time()
    result = pdf_qa({"question": query})
    end_time = time.time()
    response_time = end_time - start_time

    print(f"Answer: {result['answer']}")
    print(f"Response Time: {response_time} seconds")